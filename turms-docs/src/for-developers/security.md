# 安全

## 客户端安全

出于安全原因，本文不对Turms暂未提供专门抵御机制的CC攻击进行说明。

### 客户端黑名单机制

#### 服务端对封禁客户端的处理

当turms-gateway检测到有新的IP或用户ID被封禁时，会首先向已建立且被封禁的会话发送Turms业务层的关闭通知，该通知带有`USER_IS_BLOCKED`状态码，告知客户端它被封禁了。当数据Flush之后，Turms服务端再自动断开底层TCP连接。

当turms-gateway检查到新建立的TCP连接的对端IP已被封禁，或检测到发送登录请求的用户ID已被封禁，则在默认情况下turms-gateway会直接关闭与其的TCP连接，并且不会发送连接关闭原因的通知，如“您的IP/User ID已被封禁XX时间”。

其中有两点需要注意：

* turms-gateway自身无法在TCP连接建立之前，拒绝与被封禁的IP进行连接。如果您希望在TCP握手之前，服务端就能拒绝对被封禁的IP进行连接，您可通过Turms之后提供的：封禁用户时的回调插件，来通知云服务安全系统封禁IP，从而彻底实现IP封禁。

  另外，我们之所以不调用系统服务来彻底封禁IP，这是因为：服务端被强制关闭时，被封禁的IP将不会被自动移除；自行修改底层网络配置可能会和云服务自身的网络管理服务发生冲突，造成服务器异常。

* 在客户端连接或登陆时，turms-gateway会主动断开与封禁的IP或用户的连接，但是并不会发送连接关闭原因的通知。这么做的好处是：1. 云服务的带宽是按出网带宽收费的，入网带宽不收费，因此turms-gateway不发送业务层上的响应，可以减缓被DDoS攻击时带来的带宽费用开销；2. 减少信息暴露，尽量不要给黑客提供有效信息

#### 自动封禁机制

目前支持自动检测并封禁客户端的时机有：

* 当用户发送请求频繁，并达到一定次数时
* 当用户发送的WebSocket帧不符合规范或过大，并达到一定次数时。请求的大小依据WebSocket Frame Header中的Payload Length值
* 当用户发送的Turms客户端请求无法解析或过大，并达到一定次数时。请求的大小依据TCP字节流中客户端请求Header的Payload Length值

  补充：
  
  * 服务端检测到数据帧或客户端请求“过大”时，不会继续解析其后续的Payload部分。如果客户端的Payload Length与实际Payload长度不符，则判定为非法请求
  * 具体请求大小限制可通过`turms.gateway.client-api.max-request-size-bytes`配置

换言之，在TCP连接建立后，用户的任何行为都可能触发封禁。

Turms的自动封禁机制采用分级制度，默认提供3个等级，这3个等级的封禁时长分别是：1分钟、30分钟、60分钟。默认配置下，当客户端触发5次非法行为，则服务端会以等级1的配置封禁客户端的IP与用户ID，如果在封禁时间内，又触发了一定次数的非法行为，则进入下一个封禁等级，以此类推。

如果您想要修改默认配置，您可以通过`turms.security.blocklist.ip.auto-block`与``turms.security.blocklist.user-id.auto-block``前缀，并配合IDEA的智能提示对默认配置进行修改。其具体的配置项声明在`im.turms.server.common.property.env.common.security.AutoBlockItemProperties`类中。

#### 封禁相关API

管理员可以通过API：`/blocked-clients/ips`与`/blocked-clients/users`，分别对封禁IP与封禁用户ID做增删改查操作，具体操作遵循[Turms HTTP接口设计的一般规则](https://turms-im.github.io/docs/for-developers/admin-api.html#%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1%E5%87%86%E5%88%99)，故不赘述。

#### 封禁实现原理（拓展知识）

封禁客户端数据的同步实现原理与常见的分布式Replicated Map实现类似。即每个服务端都持有该Map的弱一致的副本，又有一个或多个Redis服务端存有一个基准副本，并且还记录了每个封禁与解封行为的logs，用于各服务端做增量同步。当新服务端上线或某服务端本地logs数据滞后100,000个记录时，这些服务端会向Redis请求全量同步，否则服务端只需以默认的10秒时间间隔向Redis请求增量logs以同步本地副本。

另外Turms目前采用的因果一致性实现是：封禁与解封动作的先后顺序以在Redis的封禁logs队列的插入顺序为基准，各服务端基于该队列的logs顺序，进行因果同步，保证封禁客户端数据的最终一致性。

### 客户端接口防刷限流

turms-gateway的限流实现采用的是主流算法`令牌桶算法`（如AWS的API Gateway提供流量整型实现就用的是令牌桶算法）。

#### 基础知识

无论什么算法，其根本都需要计算“被允许的请求数”，下文为统一说明，均用“令牌”（Token）一词指代“被允许的请求数”。另外，下表为该类算法的一般实现，其变种并不会影响其算法的本质，故不进行讨论。

|                    | 固定时间窗口算法                                             | 滑动时间窗口算法                                             | 令牌桶算法                                                   | 漏桶算法                                                     |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 令牌上限           | 固定或动态令牌上限（通常固定上限）                           | 固定或动态令牌上限（通常固定上限）                           | 固定或动态令牌上限（通常固定上限）                           | 固定或动态令牌上限（通常固定上限）                           |
| 当前可用令牌数     | 通过单个时间区间来计算                                       | 通过多个时间区间来计算                                       | 通过当前存量令牌数来计算                                     | 通过当前存量令牌数计算                                       |
| 令牌发放间隔       | 强调粗颗粒度间隔发放（如间隔1分钟）                          | 强调细颗粒度间隔发放（如间隔15秒）                           | 强调细颗粒度间隔发放（如间隔1秒）                            | 强调细颗粒度间隔放行（如间隔1秒）                            |
| 令牌发放时清空计数 | 是                                                           | 是。但一般只对最早的几个窗口进行清空                         | 否                                                           | 否                                                           |
| 资源开销           | 无需定时器，开销极小                                         | 无需定时器，开销极小                                         | 无需定时器，开销极小                                         | 每个会话都需要维护一个MPSC同步队列，与一个定时器来定时Poll队列，开销很大 |
| 实现难度           | 非常简单                                                     | 非常简单                                                     | 非常简单                                                     | 相对麻烦                                                     |
| 总评               | 由于需要清空计数，且颗粒太大，客户端可以在每次令牌发放前突发大量请求，造成“双倍突发流量”的问题 | 避免了“双倍突发流量”的问题，但因为有“清空计数”的操作，所以其控制精度不如令牌桶算法与漏桶算法 | 既可以通过存量令牌来处理突发请求，<br />又可以通过细颗粒度间隔的令牌发放来平滑地对请求进行限流。<br />其实云服务的CPU积分机制就与此类似 | 篇幅略长，见下文                                             |

漏桶算法与令牌桶算法都具有处理突发请求与平滑地对请求进行限流的能力。但漏桶算法的一个特别作用就是能对下游服务（最主要的就是数据库）进行限流。但对下游进行限流也是有代价的，它要求运维人员能够精准地估算下游服务吞吐量，否则可能造成下游服务一边处于空闲状态，上游服务却在限流的情况。

另外利用MPSC队列缓存请求，既降低了吞吐量，增加了内存开销与GC次数，导致常规用户体验更差，并加剧了DDoS攻击效果，这与我们引入防刷限流的目的背道而驰。（补充：通过阅读Turms服务端源码，您会发现Turms在处理客户端请求的流程中，代码都尽可能极致地“轻”，因此对每个用户会话都使用MPSC队列算是很重的操作了）

综上，Turms服务端最终使用`令牌桶算法`。

特别一提的是：相比于传统HTTP服务端，其接收并处理一次常规HTTP请求与响应的CPU与内存所需系统资源可能百倍于Turms服务端与其客户端交互所需系统资源（如：除开网络层协议头，Turms客户端一个请求的平均大小约32B）。因此并不需要把少部分用户的突发Turms客户端请求太当回事，可能处理上百个Turms客户端请求所用系统资源就跟处理一个HTTP请求差不多（当然，还有其他形态的CC攻击会造成大量资源消耗）。

其他：

* turms-gateway不支持并且目前也没计划支持全局的限流实现，原因是：全局限流通常是过度设计，全局限流为了时刻缓解DDoS攻击，增加Redis故障点，拉低整个系统的请求处理吞吐量，很多时候顾此失彼，得不偿失
* Turms暂不支持给不同类型的请求赋予不同的权重，如登录请求需要3个令牌，发送消息请求需要1个令牌
* turms-gateway支持运行时零停机更新令牌桶算法的配置

## 用户信息安全

对于大部分国内稍微有些网龄的群体，除非其具有很强的安全意识，他们的明文密码极有可能已经泄漏了（具体内容可以通过`社工库`进行了解）。结合大部分用户使用的密码都比较固定，因此不管服务端再怎么加密，其实“密码”的安全性还是偏低。

TODO

## 管理员安全

### 管理员认证与授权

TODO

### 管理员API接口安全

TODO

## 日志脱敏

TODO

